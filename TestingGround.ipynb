{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestingGround.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLJjqqvCg2yM"
      },
      "source": [
        "# DistilBERT Model - Sequence Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skrsRQ_gZih_",
        "outputId": "71548115-6c96-4871-a439-50d244869d3f"
      },
      "source": [
        "!pip install transformers -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 20.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 30 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 40 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 51 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 61 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 71 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 81 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 92 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 102 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 112 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 122 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 127 kB 7.5 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC1Od05PaNfP",
        "outputId": "015edb5d-f696-4d43-a0e3-58a234a6695b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDGB7-d_aXAZ",
        "outputId": "c54bca72-948c-4307-fdb1-3891c889bc52"
      },
      "source": [
        "!ls /content/drive/MyDrive/HackathonGroup4/checkpoint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config.json  tf_model.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzdNgsfpZbIQ",
        "outputId": "ad98afcf-acbf-453f-fcfc-0c540b6b2b00"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import TFDistilBertForSequenceClassification, TFTrainingArguments, TFTrainer\n",
        "from tensorflow.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',          \n",
        "    num_train_epochs=20,              \n",
        "    per_device_train_batch_size=8,  \n",
        "    per_device_eval_batch_size=8,   \n",
        "    weight_decay=0.01,    \n",
        "    eval_steps=10,\n",
        "    evaluation_strategy='steps',          \n",
        ")\n",
        "\n",
        "with training_args.strategy.scope():\n",
        "  new_model = TFDistilBertForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/HackathonGroup4/checkpoint\", num_labels=10)\n",
        "\n",
        "new_trainer = TFTrainer(model=new_model, args=training_args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at /content/drive/MyDrive/HackathonGroup4/checkpoint were not used when initializing TFDistilBertForSequenceClassification: ['dropout_199']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/HackathonGroup4/checkpoint and are newly initialized: ['dropout_19']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer_tf.py:114: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/master/examples/tensorflow\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGpIw69FZse7",
        "outputId": "11b91f55-8fc1-4b23-9340-a8b7d8e3b038"
      },
      "source": [
        "#@title test your predictions here! \\o/\n",
        "\n",
        "text = 'call ernest phone' #@param {type:\"string\"}\n",
        "\n",
        "mapper = {0: 'orbit', 2: 'enghub', 7: 'techrisk', 8: 'teutr', 6: 'mail', 5: 'kerb', 9: 'time', 3: 'help', 4: 'im', 1: 'call'}\n",
        "input_encodings = tokenizer([text], truncation=True, padding=True)\n",
        "input_dataset = Dataset.from_tensor_slices((dict(input_encodings), [0]))\n",
        "input_predictions = new_trainer.predict(input_dataset)\n",
        "\n",
        "print(input_predictions)\n",
        "print(\"Predicted Command:\", mapper[np.argmax(input_predictions.predictions)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f6920f99d70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f6920f99d70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f693c14b170> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f693c14b170> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "PredictionOutput(predictions=array([[-0.80149955,  5.845035  , -1.2270314 , -0.88215435, -0.43545786,\n",
            "        -2.4655743 , -1.2417908 , -2.0422313 , -1.9202375 , -1.5429876 ]],\n",
            "      dtype=float32), label_ids=array([0], dtype=int32), metrics={'eval_loss': 6.65422248840332})\n",
            "Predicted Command: call\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocKDcSovg6vm"
      },
      "source": [
        "# Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdbvQa4ydcAI",
        "outputId": "ff5d62f0-77cc-495d-9f0f-e2b47dd686ae"
      },
      "source": [
        "!pip install boto3\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "print(nltk.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.18.6-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting botocore<1.22.0,>=1.21.6\n",
            "  Downloading botocore-1.21.6-py3-none-any.whl (7.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7 MB 8.8 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 45.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.6->boto3) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.6->boto3) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.6 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.18.6 botocore-1.21.6 jmespath-0.10.0 s3transfer-0.5.0 urllib3-1.26.6\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "3.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMX2KJwEdM0g",
        "outputId": "3c6f59eb-75a1-4247-8a4d-5f43b87ab165"
      },
      "source": [
        "def identify_name_location(command):\n",
        "  command = \"how to \"+command\n",
        "  command = command.title()\n",
        "  \n",
        "  ne_tree = nltk.ne_chunk(pos_tag(word_tokenize(command)))\n",
        "  for node in ne_tree:\n",
        "    if(type(node) == nltk.tree.Tree and (node.label() == 'PERSON' or node.label() == 'GPE')):\n",
        "      name = \"\"\n",
        "      for child in node:\n",
        "        name+=child[0] + \" \"\n",
        "      return name.rstrip()\n",
        "\n",
        "identify_name_location('call ernest phone')\n",
        "print(identify_name_location(\"chat with binitha\"))\n",
        "print(identify_name_location(\"how to email siu hong?\")) # Failing test case\n",
        "print(identify_name_location(\"what is callista's kerberos?\"))\n",
        "print(identify_name_location(\"what is the time now in london?\"))\n",
        "print(identify_name_location(\"what is bengaluru's time?\"))\n",
        "print(identify_name_location(\"what is for lunch in hong kong?\")) # Failing test case\n",
        "\n",
        "print(identify_name_location(\"skype callista\"))\n",
        "print(identify_name_location(\"call siu hong\"))\n",
        "print(identify_name_location(\"kerberos pengfei\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Binitha\n",
            "Email Siu Hong\n",
            "Callista\n",
            "London\n",
            "Bengaluru\n",
            "Lunch\n",
            "Callista\n",
            "Siu Hong\n",
            "Pengfei\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3NthWQQdWxc",
        "outputId": "39ab10ca-fe4a-4273-e357-9c01e3d366e4"
      },
      "source": [
        "import boto3\n",
        "\n",
        "def retrieve_kerberos(name, access_key, secret_access_key):\n",
        "\n",
        "  if(not name):\n",
        "    return \"\"\n",
        "\n",
        "  # Get the service resource.\n",
        "  client = boto3.client(\n",
        "      'dynamodb',\n",
        "      aws_access_key_id=access_key,\n",
        "      aws_secret_access_key=secret_access_key,\n",
        "      region_name = 'us-east-2'\n",
        "    )\n",
        "\n",
        "  response = client.scan(\n",
        "        TableName='kerbuser',\n",
        "        Select='ALL_ATTRIBUTES'\n",
        "    )\n",
        "  \n",
        "  users = response['Items']\n",
        "  for user in users:\n",
        "    if(name.lower() in user['name']['S'].lower()):\n",
        "      return user['kerberos']['S']\n",
        "\n",
        "  return \"\"\n",
        "\n",
        "access_key = \"\"\n",
        "secret_access_key = \"\"\n",
        "print(retrieve_kerberos(\"Callista\",access_key,secret_access_key))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chcall\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ca6ZumwPjl2"
      },
      "source": [
        "def retrieve_kerberos(name):\n",
        "\n",
        "  if(not name):\n",
        "    return \"\"\n",
        "\n",
        "  # Get the service resource.\n",
        "  sts_client = boto3.client('sts')\n",
        "  sts_client_response = sts_client.assume_role(RoleArn=ROLE_ARN,RoleSessionName=\"lambdaSession\")\n",
        "\n",
        "  db = boto3.client(\n",
        "  'dynamodb',\n",
        "  aws_access_key_id=sts_client_response[\"Credentials\"][\"AccessKeyId\"],\n",
        "  aws_secret_access_key=sts_client_response[\"Credentials\"][\"SecretAccessKey\"],\n",
        "  aws_session_token=sts_client_response[\"Credentials\"][\"SessionToken\"])\n",
        "       \n",
        "\n",
        "  response = db.scan(\n",
        "        TableName='kerbuser',\n",
        "        Select='ALL_ATTRIBUTES'\n",
        "    )\n",
        "  \n",
        "  users = response['Items']\n",
        "  for user in users:\n",
        "    if(name.lower() in user['name']['S'].lower()):\n",
        "      return user['kerberos']['S']\n",
        "  return \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i13Lv6pFe1tE",
        "outputId": "cb10c6ff-6322-47e1-ea79-b9d79cfb58e5"
      },
      "source": [
        "def identify_query(command):\n",
        "  command = \"how to \"+command\n",
        "  command = command.lower()\n",
        "  ne_tree = nltk.ne_chunk(pos_tag(word_tokenize(command)))\n",
        "  word_list = []\n",
        "  for nodes in ne_tree.pos():\n",
        "    for node in nodes:\n",
        "      if(type(node) == tuple and (node[1] == 'NN' or node[1] == 'VB')):\n",
        "          word_list.append(node[0])\n",
        "    \n",
        "  return \" \".join(word_list)\n",
        "\n",
        "print(identify_query(\"How to reinstall my browser?\"))\n",
        "print(identify_query(\"How to resolve gs app store issues?\"))\n",
        "print(identify_query(\"How to reboot vscode?\"))\n",
        "print(identify_query(\"How to add wordart on powerpoint?\"))\n",
        "print(identify_query(\"How to send zoom invite through outlook\"))\n",
        "print(identify_query(\"How to split a document in microsoft word\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reinstall browser\n",
            "resolve store\n",
            "reboot vscode\n",
            "add wordart powerpoint\n",
            "send zoom invite outlook\n",
            "split document word\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}